{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit Contractions\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of products and code\n",
    "products = ['esheath','ev1000', 'commander delivery system v2.1', 'main assembly', 'dilator kit (4 piece) with nf', 'icf100', 'clearsight cuff', 'certitude delivery system v2', 'rc', 'balloons and extrusions', 'not applicable', 'pr emb', 'axela sheath', 'physio i', 'inspiris', 'dr pressure monitoring', 'ultra delivery system', 'dr pressure tubing', 'centera delivery system', 'arterials', 'cardioband anchor drives', 'introcsc', 'ascendra+ delivery system', 'pascal implant system', 'fems', 'pascal guide sheath', 'sapien 3', 'sapien 3 prime', 'universal crimper', 'clearsight hrs', 'pr pac & dnzig', 'dr accessories', '9850tmv edwards transcatheter mitral valve', 'pr bi polar', 'ascendra bavc', 'intuity', 'pr non pvc, td & torque', 'hemosphere', 'vigileo', 'pr cco', 'guidewire delivery catheter', 'expandable sheath', 'qd', 'dr vamp adult system', 'main mis', 'physio ii', 'alterra prestent', 'cardioband implant', 'sapien 3 ultra', 'classic', 'pr fogarty clamps', 'infus pressure monitoring', 'perimount aortic', 'dr flush device', 'cardioband delivery system', 'novaflex + delivery system', 'pr chandler', 'pr mod iii', 'physio tricuspid', 'alterra delivery system', 'crimper (sapien)', 'pr specialty', 'ev1000 databox', 'magna mitral ease', 'certitude sheath set v1', 'pr thrulum', 'latex balloon', 'dr flotrac system', 'magna ease', 'mc3', 'hemosphere oximetry cable', 'imr', '9850sb edwards transseptal stabilizer', 'pr vip & p port', 'cosgrove', '9850ts edwards transseptal delivery system', 'ev1000 panel', 'fore-sight elite and fore-sight', 'tf balloon catheter (pre-dilation)', 'atrial shunt', 'multi-pack', 'clearsight', 'dr vamp plus system','0084718','SOP5739R']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyenchant\n",
    "# !pip install unicodedata\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "\n",
    "def clean_text(text,format):\n",
    "    text_lower = convert_to_lowercase(text)\n",
    "    text_rem_acc = strip_accents(text_lower)\n",
    "    text_token = tokenizer(text_rem_acc)\n",
    "    text_post_contr = contract(text_token)\n",
    "    text_post_punct = punctuation(text_post_contr)\n",
    "    text_eng_word  = english_words(text_post_punct)\n",
    "    text_stop = stop_words(text_eng_word)\n",
    "    text_doc_wd = doc_words(text_stop)\n",
    "    clean_text = lemmatizer(text_doc_wd, format)\n",
    "    return clean_text\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#Convert Text to lowercase\n",
    "def convert_to_lowercase(text): \n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Replace letters with accents\n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)\n",
    "\n",
    "#Split each word in a text\n",
    "def tokenizer(text): \n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    text = tokenizer.tokenize(text)\n",
    "    return text\n",
    "\n",
    "#Break down contractions\n",
    "def contract(text):\n",
    "    text = [contractions[word] if word in contractions else word for word in text]\n",
    "    return text\n",
    "\n",
    "#Remove non-english words\n",
    "def english_words(text):\n",
    "    import enchant\n",
    "    eng_words = enchant.Dict('en_US')\n",
    "    text = [word for word in text if word != '' and eng_words.check(word.upper()) and word.isalpha() \n",
    "            or word in products or re.match(r'[A-Za-z]+\\d+',word)]\n",
    "    return text\n",
    "\n",
    "#Remove any unneccesary punctuation\n",
    "def punctuation(text):\n",
    "    import re\n",
    "    text = [re.sub('['+string.punctuation+']', '', word) for word in text]\n",
    "    return text\n",
    "\n",
    "#Remove any stop words \n",
    "def stop_words(text):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stop]\n",
    "    return text\n",
    "\n",
    "#Fix Document Words\n",
    "def doc_words(text):\n",
    "    text = [word.upper() if re.match(r'[A-Za-z]+\\d+',word) else word for word in text]\n",
    "    return text\n",
    "\n",
    "#Convert each word to its based form\n",
    "def lemmatizer(text,format=string):\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    if format == string:\n",
    "        text = \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "    if format == list:\n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('NCR_PRA_Description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.applymap(lambda x: clean_text(x,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.replace('',np.NaN, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv('clean_NCR_PRA_Description.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
