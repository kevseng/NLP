{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit Contractions\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyenchant\n",
    "# !pip install unicodedata\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')\n",
    "\n",
    "\n",
    "def clean_text(text,format):\n",
    "    text_lower = convert_to_lowercase(text)\n",
    "    text_rem_acc = strip_accents(text_lower)\n",
    "    text_token = tokenizer(text_rem_acc)\n",
    "    text_post_contr = contract(text_token)\n",
    "    text_post_punct = punctuation(text_post_contr)\n",
    "    text_eng_word  = english_words(text_post_punct)\n",
    "    text_stop = stop_words(text_eng_word)\n",
    "    text_doc_wd = doc_words(text_stop)\n",
    "    clean_text = lemmatizer(text_doc_wd, format)\n",
    "    return clean_text\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "#Convert Text to lowercase\n",
    "def convert_to_lowercase(text): \n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Replace letters with accents\n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)\n",
    "\n",
    "#Split each word in a text\n",
    "def tokenizer(text): \n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    text = tokenizer.tokenize(text)\n",
    "    return text\n",
    "\n",
    "#Break down contractions\n",
    "def contract(text):\n",
    "    text = [contractions[word] if word in contractions else word for word in text]\n",
    "    return text\n",
    "\n",
    "#Remove non-english words\n",
    "def english_words(text):\n",
    "    import enchant\n",
    "    eng_words = enchant.Dict('en_US')\n",
    "    text = [word for word in text if word != '' and eng_words.check(word.upper()) and word.isalpha() \n",
    "            or word in products or re.match(r'[A-Za-z]+\\d+',word)]\n",
    "    return text\n",
    "\n",
    "#Remove any unneccesary punctuation\n",
    "def punctuation(text):\n",
    "    import re\n",
    "    text = [re.sub('['+string.punctuation+']', '', word) for word in text]\n",
    "    return text\n",
    "\n",
    "#Remove any stop words \n",
    "def stop_words(text):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stop]\n",
    "    return text\n",
    "\n",
    "#Highlight Document Words\n",
    "def doc_words(text):\n",
    "    text = [word.upper() if re.match(r'[A-Za-z]+\\d+',word) else word for word in text]\n",
    "    return text\n",
    "\n",
    "#Convert each word to its based form\n",
    "def lemmatizer(text,format=string):\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    if format == string:\n",
    "        text = \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "    if format == list:\n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
